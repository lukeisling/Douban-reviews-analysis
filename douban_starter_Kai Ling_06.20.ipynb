{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 豆瓣评分的预测\n",
    "\n",
    "在这个项目中，我们要预测一部电影的评分，这个问题实际上就是一个分类问题。给定的输入为一段文本，输出为具体的评分。 在这个项目中，我们需要做：\n",
    "- 文本的预处理，如停用词的过滤，低频词的过滤，特殊符号的过滤等\n",
    "- 文本转化成向量，将使用三种方式，分别为tf-idf, word2vec以及BERT向量。 \n",
    "- 训练逻辑回归和朴素贝叶斯模型，并做交叉验证\n",
    "- 评估模型的准确率\n",
    "\n",
    "在具体标记为``TODO``的部分填写相应的代码。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入数据处理的基础包\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#导入用于计数的包\n",
    "from collections import Counter\n",
    "\n",
    "#导入tf-idf相关的包\n",
    "from sklearn.feature_extraction.text import TfidfTransformer    \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#导入模型评估的包\n",
    "from sklearn import metrics\n",
    "\n",
    "#导入与word2vec相关的包\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "#导入与bert embedding相关的包，关于mxnet包下载的注意事项参考实验手册\n",
    "from bert_embedding import BertEmbedding\n",
    "import mxnet\n",
    "\n",
    "#包tqdm是用来对可迭代对象执行时生成一个进度条用以监视程序运行过程\n",
    "from tqdm import tqdm\n",
    "\n",
    "#导入其他一些功能包\n",
    "import requests\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 读取数据并做文本的处理\n",
    "你需要完成以下几步操作：\n",
    "- 去掉无用的字符如！&，可自行定义\n",
    "- 中文分词\n",
    "- 去掉低频词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Movie_Name_EN</th>\n",
       "      <th>Movie_Name_CN</th>\n",
       "      <th>Crawl_Date</th>\n",
       "      <th>Number</th>\n",
       "      <th>Username</th>\n",
       "      <th>Date</th>\n",
       "      <th>Star</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Like</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Avengers Age of Ultron</td>\n",
       "      <td>复仇者联盟2</td>\n",
       "      <td>2017-01-22</td>\n",
       "      <td>1</td>\n",
       "      <td>然潘</td>\n",
       "      <td>2015-05-13</td>\n",
       "      <td>3</td>\n",
       "      <td>连奥创都知道整容要去韩国。</td>\n",
       "      <td>2404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>Avengers Age of Ultron</td>\n",
       "      <td>复仇者联盟2</td>\n",
       "      <td>2017-01-22</td>\n",
       "      <td>11</td>\n",
       "      <td>影志</td>\n",
       "      <td>2015-04-30</td>\n",
       "      <td>4</td>\n",
       "      <td>“一个没有黑暗面的人不值得信任。” 第二部剥去冗长的铺垫，开场即高潮、一直到结束，会有人觉...</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>Avengers Age of Ultron</td>\n",
       "      <td>复仇者联盟2</td>\n",
       "      <td>2017-01-22</td>\n",
       "      <td>21</td>\n",
       "      <td>随时流感</td>\n",
       "      <td>2015-04-28</td>\n",
       "      <td>2</td>\n",
       "      <td>奥创弱爆了弱爆了弱爆了啊！！！！！！</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>Avengers Age of Ultron</td>\n",
       "      <td>复仇者联盟2</td>\n",
       "      <td>2017-01-22</td>\n",
       "      <td>31</td>\n",
       "      <td>乌鸦火堂</td>\n",
       "      <td>2015-05-08</td>\n",
       "      <td>4</td>\n",
       "      <td>与第一集不同，承上启下，阴郁严肃，但也不会不好看啊，除非本来就不喜欢漫威电影。场面更加宏大...</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40</td>\n",
       "      <td>Avengers Age of Ultron</td>\n",
       "      <td>复仇者联盟2</td>\n",
       "      <td>2017-01-22</td>\n",
       "      <td>41</td>\n",
       "      <td>办公室甜心</td>\n",
       "      <td>2015-05-10</td>\n",
       "      <td>5</td>\n",
       "      <td>看毕，我激动地对友人说，等等奥创要来毁灭台北怎么办厚，她拍了拍我肩膀，没事，反正你买了两份...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID           Movie_Name_EN Movie_Name_CN  Crawl_Date  Number Username  \\\n",
       "0   0  Avengers Age of Ultron        复仇者联盟2  2017-01-22       1       然潘   \n",
       "1  10  Avengers Age of Ultron        复仇者联盟2  2017-01-22      11       影志   \n",
       "2  20  Avengers Age of Ultron        复仇者联盟2  2017-01-22      21     随时流感   \n",
       "3  30  Avengers Age of Ultron        复仇者联盟2  2017-01-22      31     乌鸦火堂   \n",
       "4  40  Avengers Age of Ultron        复仇者联盟2  2017-01-22      41    办公室甜心   \n",
       "\n",
       "         Date  Star                                            Comment  Like  \n",
       "0  2015-05-13     3                                      连奥创都知道整容要去韩国。  2404  \n",
       "1  2015-04-30     4   “一个没有黑暗面的人不值得信任。” 第二部剥去冗长的铺垫，开场即高潮、一直到结束，会有人觉...   381  \n",
       "2  2015-04-28     2                                 奥创弱爆了弱爆了弱爆了啊！！！！！！   120  \n",
       "3  2015-05-08     4   与第一集不同，承上启下，阴郁严肃，但也不会不好看啊，除非本来就不喜欢漫威电影。场面更加宏大...    30  \n",
       "4  2015-05-10     5   看毕，我激动地对友人说，等等奥创要来毁灭台北怎么办厚，她拍了拍我肩膀，没事，反正你买了两份...    16  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#读取数据\n",
    "data = pd.read_csv('data/DMSC.csv')\n",
    "#观察数据格式\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#输出数据的一些相关信息\n",
    "#data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>Star</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>连奥创都知道整容要去韩国。</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>“一个没有黑暗面的人不值得信任。” 第二部剥去冗长的铺垫，开场即高潮、一直到结束，会有人觉...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>奥创弱爆了弱爆了弱爆了啊！！！！！！</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>与第一集不同，承上启下，阴郁严肃，但也不会不好看啊，除非本来就不喜欢漫威电影。场面更加宏大...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>看毕，我激动地对友人说，等等奥创要来毁灭台北怎么办厚，她拍了拍我肩膀，没事，反正你买了两份...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Comment  Star\n",
       "0                                      连奥创都知道整容要去韩国。     3\n",
       "1   “一个没有黑暗面的人不值得信任。” 第二部剥去冗长的铺垫，开场即高潮、一直到结束，会有人觉...     4\n",
       "2                                 奥创弱爆了弱爆了弱爆了啊！！！！！！     2\n",
       "3   与第一集不同，承上启下，阴郁严肃，但也不会不好看啊，除非本来就不喜欢漫威电影。场面更加宏大...     4\n",
       "4   看毕，我激动地对友人说，等等奥创要来毁灭台北怎么办厚，她拍了拍我肩膀，没事，反正你买了两份...     5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#只保留数据中我们需要的两列：Comment列和Star列\n",
    "data = data[['Comment','Star']]\n",
    "#观察新的数据的格式\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>Star</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>连奥创都知道整容要去韩国。</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>“一个没有黑暗面的人不值得信任。” 第二部剥去冗长的铺垫，开场即高潮、一直到结束，会有人觉...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>奥创弱爆了弱爆了弱爆了啊！！！！！！</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>与第一集不同，承上启下，阴郁严肃，但也不会不好看啊，除非本来就不喜欢漫威电影。场面更加宏大...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>看毕，我激动地对友人说，等等奥创要来毁灭台北怎么办厚，她拍了拍我肩膀，没事，反正你买了两份...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212501</th>\n",
       "      <td>里里外外我都打满分，太赞了。</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212502</th>\n",
       "      <td>超棒！！！拟人超级像的，每个配角都好有戏，每个动物都好萌，想摸摸毛，fox一直叫bunny...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212503</th>\n",
       "      <td>狐狸确实帅</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212504</th>\n",
       "      <td>不负我望，超级好看！</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212505</th>\n",
       "      <td>毛——茸茸——的——胜利——</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>212506 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Comment  Star\n",
       "0                                           连奥创都知道整容要去韩国。     1\n",
       "1        “一个没有黑暗面的人不值得信任。” 第二部剥去冗长的铺垫，开场即高潮、一直到结束，会有人觉...     1\n",
       "2                                      奥创弱爆了弱爆了弱爆了啊！！！！！！     0\n",
       "3        与第一集不同，承上启下，阴郁严肃，但也不会不好看啊，除非本来就不喜欢漫威电影。场面更加宏大...     1\n",
       "4        看毕，我激动地对友人说，等等奥创要来毁灭台北怎么办厚，她拍了拍我肩膀，没事，反正你买了两份...     1\n",
       "...                                                   ...   ...\n",
       "212501                                     里里外外我都打满分，太赞了。     1\n",
       "212502   超棒！！！拟人超级像的，每个配角都好有戏，每个动物都好萌，想摸摸毛，fox一直叫bunny...     1\n",
       "212503                                              狐狸确实帅     1\n",
       "212504                                         不负我望，超级好看！     1\n",
       "212505                                     毛——茸茸——的——胜利——     1\n",
       "\n",
       "[212506 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这里的star代表具体的评分。但在这个项目中，我们要预测的是正面还是负面。我们把评分为1和2的看作是负面，把评分为3，4，5的作为正面\n",
    "data['Star']=(data.Star/3).astype(int)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 任务1： 去掉一些无用的字符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO1: 去掉一些无用的字符，自行定一个字符几何，并从文本中去掉\n",
    "#    your to do \n",
    "import re\n",
    "\n",
    "# Remove punctuation\n",
    "data['Comment'] = data['Comment'].apply(lambda x: re.sub(r'[，。！？￥&—\\“”、@.·~]', ' ', x))\n",
    "\n",
    "# Remove space\n",
    "data['Comment'] = data['Comment'] .apply(lambda x: re.sub(r\"\\s+\", \"\", x))\n",
    "\n",
    "# add English chars flag\n",
    "data['Comment'] = data['Comment'] .apply(lambda x: re.sub(r\"^[A-Za-z]+$\", ' 英文 ', x))\n",
    "\n",
    "# Add num flag\n",
    "data['Comment'] = data['Comment'].apply(lambda x: re.sub('[0-9]+', ' 数字 ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>Star</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>连奥创都知道整容要去韩国</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>一个没有黑暗面的人不值得信任第二部剥去冗长的铺垫开场即高潮一直到结束会有人觉得只剩动作特技不...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>奥创弱爆了弱爆了弱爆了啊</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>与第一集不同承上启下阴郁严肃但也不会不好看啊除非本来就不喜欢漫威电影场面更加宏大单打与团战又...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>看毕我激动地对友人说等等奥创要来毁灭台北怎么办厚她拍了拍我肩膀没事反正你买了两份旅行保险惹</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212501</th>\n",
       "      <td>里里外外我都打满分太赞了</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212502</th>\n",
       "      <td>超棒拟人超级像的每个配角都好有戏每个动物都好萌想摸摸毛fox一直叫bunny胡萝卜这点超级萌...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212503</th>\n",
       "      <td>狐狸确实帅</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212504</th>\n",
       "      <td>不负我望超级好看</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212505</th>\n",
       "      <td>毛茸茸的胜利</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>212506 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Comment  Star\n",
       "0                                            连奥创都知道整容要去韩国     1\n",
       "1       一个没有黑暗面的人不值得信任第二部剥去冗长的铺垫开场即高潮一直到结束会有人觉得只剩动作特技不...     1\n",
       "2                                            奥创弱爆了弱爆了弱爆了啊     0\n",
       "3       与第一集不同承上启下阴郁严肃但也不会不好看啊除非本来就不喜欢漫威电影场面更加宏大单打与团战又...     1\n",
       "4           看毕我激动地对友人说等等奥创要来毁灭台北怎么办厚她拍了拍我肩膀没事反正你买了两份旅行保险惹     1\n",
       "...                                                   ...   ...\n",
       "212501                                       里里外外我都打满分太赞了     1\n",
       "212502  超棒拟人超级像的每个配角都好有戏每个动物都好萌想摸摸毛fox一直叫bunny胡萝卜这点超级萌...     1\n",
       "212503                                              狐狸确实帅     1\n",
       "212504                                           不负我望超级好看     1\n",
       "212505                                             毛茸茸的胜利     1\n",
       "\n",
       "[212506 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 任务2：使用结巴分词对文本做分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "apply:   0%|          | 0/212506 [00:00<?, ?it/s]Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ling\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.762 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "apply: 100%|██████████| 212506/212506 [00:39<00:00, 5351.59it/s]\n"
     ]
    }
   ],
   "source": [
    "# TODO2: 导入中文分词包jieba, 并用jieba对原始文本做分词\n",
    "import jieba\n",
    "def comment_cut(content):\n",
    "    seg = jieba.cut(content, cut_all=False)\n",
    "    return list(seg)\n",
    "\n",
    "# 输出进度条\n",
    "tqdm.pandas(desc='apply')\n",
    "data['comment_processed'] = data['Comment'].progress_apply(comment_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>Star</th>\n",
       "      <th>comment_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>连奥创都知道整容要去韩国</td>\n",
       "      <td>1</td>\n",
       "      <td>[连, 奥创, 都, 知道, 整容, 要, 去, 韩国]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>一个没有黑暗面的人不值得信任第二部剥去冗长的铺垫开场即高潮一直到结束会有人觉得只剩动作特技不...</td>\n",
       "      <td>1</td>\n",
       "      <td>[一个, 没有, 黑暗面, 的, 人, 不, 值得, 信任, 第二部, 剥去, 冗长, 的,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>奥创弱爆了弱爆了弱爆了啊</td>\n",
       "      <td>0</td>\n",
       "      <td>[奥创, 弱, 爆, 了, 弱, 爆, 了, 弱, 爆, 了, 啊]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>与第一集不同承上启下阴郁严肃但也不会不好看啊除非本来就不喜欢漫威电影场面更加宏大单打与团战又...</td>\n",
       "      <td>1</td>\n",
       "      <td>[与, 第一集, 不同, 承上启下, 阴郁, 严肃, 但, 也, 不会, 不, 好看, 啊,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>看毕我激动地对友人说等等奥创要来毁灭台北怎么办厚她拍了拍我肩膀没事反正你买了两份旅行保险惹</td>\n",
       "      <td>1</td>\n",
       "      <td>[看毕, 我, 激动, 地, 对, 友人, 说, 等等, 奥创, 要, 来, 毁灭, 台北,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Comment  Star  \\\n",
       "0                                       连奥创都知道整容要去韩国     1   \n",
       "1  一个没有黑暗面的人不值得信任第二部剥去冗长的铺垫开场即高潮一直到结束会有人觉得只剩动作特技不...     1   \n",
       "2                                       奥创弱爆了弱爆了弱爆了啊     0   \n",
       "3  与第一集不同承上启下阴郁严肃但也不会不好看啊除非本来就不喜欢漫威电影场面更加宏大单打与团战又...     1   \n",
       "4      看毕我激动地对友人说等等奥创要来毁灭台北怎么办厚她拍了拍我肩膀没事反正你买了两份旅行保险惹     1   \n",
       "\n",
       "                                   comment_processed  \n",
       "0                       [连, 奥创, 都, 知道, 整容, 要, 去, 韩国]  \n",
       "1  [一个, 没有, 黑暗面, 的, 人, 不, 值得, 信任, 第二部, 剥去, 冗长, 的,...  \n",
       "2                 [奥创, 弱, 爆, 了, 弱, 爆, 了, 弱, 爆, 了, 啊]  \n",
       "3  [与, 第一集, 不同, 承上启下, 阴郁, 严肃, 但, 也, 不会, 不, 好看, 啊,...  \n",
       "4  [看毕, 我, 激动, 地, 对, 友人, 说, 等等, 奥创, 要, 来, 毁灭, 台北,...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 观察新的数据的格式\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 任务3：设定停用词并去掉停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "apply: 100%|██████████| 212506/212506 [00:44<00:00, 4765.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# TODO3: 设定停用词并从文本中去掉停用词\n",
    "\n",
    "# 下载中文停用词表至data/stopWord.json中，下载地址:https://github.com/goto456/stopwords/\n",
    "if not os.path.exists('data/stopWord.json'):\n",
    "    stopWord = requests.get(\"https://raw.githubusercontent.com/goto456/stopwords/master/cn_stopwords.txt\")\n",
    "    with open(\"data/stopWord.json\", \"wb\") as f:\n",
    "         f.write(stopWord.content)\n",
    "\n",
    "# 读取下载的停用词表，并保存在列表中\n",
    "with open(\"data/stopWord.json\",\"r\",encoding=\"utf8\") as f:\n",
    "    stopWords = f.read().split(\"\\n\")  \n",
    "    \n",
    "    \n",
    "# 去除停用词\n",
    "def rm_stop_word(wordList):\n",
    "    filwords = [word for word in wordList if word not in stopWords]\n",
    "    return filwords\n",
    "\n",
    "#这行代码中.progress_apply()函数的作用等同于.apply()函数的作用，只是写成.progress_apply()函数才能被tqdm包监控从而输出进度条。\n",
    "data['comment_processed'] = data['comment_processed'].progress_apply(rm_stop_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>Star</th>\n",
       "      <th>comment_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>连奥创都知道整容要去韩国</td>\n",
       "      <td>1</td>\n",
       "      <td>[奥创, 知道, 整容, 韩国]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>一个没有黑暗面的人不值得信任第二部剥去冗长的铺垫开场即高潮一直到结束会有人觉得只剩动作特技不...</td>\n",
       "      <td>1</td>\n",
       "      <td>[一个, 没有, 黑暗面, 值得, 信任, 第二部, 剥去, 冗长, 铺垫, 开场, 高潮,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>奥创弱爆了弱爆了弱爆了啊</td>\n",
       "      <td>0</td>\n",
       "      <td>[奥创, 弱, 爆, 弱, 爆, 弱, 爆]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>与第一集不同承上启下阴郁严肃但也不会不好看啊除非本来就不喜欢漫威电影场面更加宏大单打与团战又...</td>\n",
       "      <td>1</td>\n",
       "      <td>[第一集, 不同, 承上启下, 阴郁, 严肃, 不会, 好看, 本来, 喜欢, 漫威, 电影...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>看毕我激动地对友人说等等奥创要来毁灭台北怎么办厚她拍了拍我肩膀没事反正你买了两份旅行保险惹</td>\n",
       "      <td>1</td>\n",
       "      <td>[看毕, 激动, 友人, 说, 奥创, 毁灭, 台北, 厚, 拍了拍, 肩膀, 没事, 反正...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Comment  Star  \\\n",
       "0                                       连奥创都知道整容要去韩国     1   \n",
       "1  一个没有黑暗面的人不值得信任第二部剥去冗长的铺垫开场即高潮一直到结束会有人觉得只剩动作特技不...     1   \n",
       "2                                       奥创弱爆了弱爆了弱爆了啊     0   \n",
       "3  与第一集不同承上启下阴郁严肃但也不会不好看啊除非本来就不喜欢漫威电影场面更加宏大单打与团战又...     1   \n",
       "4      看毕我激动地对友人说等等奥创要来毁灭台北怎么办厚她拍了拍我肩膀没事反正你买了两份旅行保险惹     1   \n",
       "\n",
       "                                   comment_processed  \n",
       "0                                   [奥创, 知道, 整容, 韩国]  \n",
       "1  [一个, 没有, 黑暗面, 值得, 信任, 第二部, 剥去, 冗长, 铺垫, 开场, 高潮,...  \n",
       "2                             [奥创, 弱, 爆, 弱, 爆, 弱, 爆]  \n",
       "3  [第一集, 不同, 承上启下, 阴郁, 严肃, 不会, 好看, 本来, 喜欢, 漫威, 电影...  \n",
       "4  [看毕, 激动, 友人, 说, 奥创, 毁灭, 台北, 厚, 拍了拍, 肩膀, 没事, 反正...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 观察新的数据的格式\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 任务4：去掉低频词，出现次数少于10次的词去掉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO4: 去除低频词, 去掉词频小于10的单词，并把结果存放在data['comment_processed']里\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "\n",
    "# split words into lists\n",
    "v = data['comment_processed'].tolist()\n",
    "\n",
    "# compute global word frequency\n",
    "c = Counter(chain.from_iterable(v))\n",
    "\n",
    "# filter, join, and re-assign\n",
    "data['comment_processed'] = [' '.join([j for j in i if c[j] > 9]) for i in v]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>Star</th>\n",
       "      <th>comment_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>连奥创都知道整容要去韩国</td>\n",
       "      <td>1</td>\n",
       "      <td>奥创 知道 整容 韩国</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>一个没有黑暗面的人不值得信任第二部剥去冗长的铺垫开场即高潮一直到结束会有人觉得只剩动作特技不...</td>\n",
       "      <td>1</td>\n",
       "      <td>一个 没有 黑暗面 值得 信任 第二部 冗长 铺垫 开场 高潮 一直 结束 会 有人 觉得 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>奥创弱爆了弱爆了弱爆了啊</td>\n",
       "      <td>0</td>\n",
       "      <td>奥创 弱 爆 弱 爆 弱 爆</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>与第一集不同承上启下阴郁严肃但也不会不好看啊除非本来就不喜欢漫威电影场面更加宏大单打与团战又...</td>\n",
       "      <td>1</td>\n",
       "      <td>第一集 不同 承上启下 阴郁 严肃 不会 好看 本来 喜欢 漫威 电影 场面 更加 宏大 团...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>看毕我激动地对友人说等等奥创要来毁灭台北怎么办厚她拍了拍我肩膀没事反正你买了两份旅行保险惹</td>\n",
       "      <td>1</td>\n",
       "      <td>激动 友人 说 奥创 毁灭 台北 厚 肩膀 没事 反正 买 两份 旅行 惹</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212501</th>\n",
       "      <td>里里外外我都打满分太赞了</td>\n",
       "      <td>1</td>\n",
       "      <td>满分 太赞</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212502</th>\n",
       "      <td>超棒拟人超级像的每个配角都好有戏每个动物都好萌想摸摸毛fox一直叫bunny胡萝卜这点超级萌...</td>\n",
       "      <td>1</td>\n",
       "      <td>超棒 拟人 超级 每个 配角 有戏 每个 动物 摸摸 毛 一直 bunny 胡萝卜 这点 超...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212503</th>\n",
       "      <td>狐狸确实帅</td>\n",
       "      <td>1</td>\n",
       "      <td>狐狸 确实 帅</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212504</th>\n",
       "      <td>不负我望超级好看</td>\n",
       "      <td>1</td>\n",
       "      <td>不负 超级 好看</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212505</th>\n",
       "      <td>毛茸茸的胜利</td>\n",
       "      <td>1</td>\n",
       "      <td>毛茸茸 胜利</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>212506 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Comment  Star  \\\n",
       "0                                            连奥创都知道整容要去韩国     1   \n",
       "1       一个没有黑暗面的人不值得信任第二部剥去冗长的铺垫开场即高潮一直到结束会有人觉得只剩动作特技不...     1   \n",
       "2                                            奥创弱爆了弱爆了弱爆了啊     0   \n",
       "3       与第一集不同承上启下阴郁严肃但也不会不好看啊除非本来就不喜欢漫威电影场面更加宏大单打与团战又...     1   \n",
       "4           看毕我激动地对友人说等等奥创要来毁灭台北怎么办厚她拍了拍我肩膀没事反正你买了两份旅行保险惹     1   \n",
       "...                                                   ...   ...   \n",
       "212501                                       里里外外我都打满分太赞了     1   \n",
       "212502  超棒拟人超级像的每个配角都好有戏每个动物都好萌想摸摸毛fox一直叫bunny胡萝卜这点超级萌...     1   \n",
       "212503                                              狐狸确实帅     1   \n",
       "212504                                           不负我望超级好看     1   \n",
       "212505                                             毛茸茸的胜利     1   \n",
       "\n",
       "                                        comment_processed  \n",
       "0                                             奥创 知道 整容 韩国  \n",
       "1       一个 没有 黑暗面 值得 信任 第二部 冗长 铺垫 开场 高潮 一直 结束 会 有人 觉得 ...  \n",
       "2                                          奥创 弱 爆 弱 爆 弱 爆  \n",
       "3       第一集 不同 承上启下 阴郁 严肃 不会 好看 本来 喜欢 漫威 电影 场面 更加 宏大 团...  \n",
       "4                   激动 友人 说 奥创 毁灭 台北 厚 肩膀 没事 反正 买 两份 旅行 惹  \n",
       "...                                                   ...  \n",
       "212501                                              满分 太赞  \n",
       "212502  超棒 拟人 超级 每个 配角 有戏 每个 动物 摸摸 毛 一直 bunny 胡萝卜 这点 超...  \n",
       "212503                                            狐狸 确实 帅  \n",
       "212504                                           不负 超级 好看  \n",
       "212505                                             毛茸茸 胜利  \n",
       "\n",
       "[212506 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 把文本分为训练集和测试集\n",
    "选择语料库中的20%作为测试数据，剩下的作为训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO5: 把数据分为训练集和测试集. comments_train（list)保存用于训练的文本，comments_test(list)保存用于测试的文本。 y_train, y_test是对应的标签（0、1）\n",
    "from sklearn.model_selection import train_test_split\n",
    "test_ratio = 0.2\n",
    "comments_train, comments_test, y_train, y_test = train_test_split(data['comment_processed'], data['Star'], test_size=0.2, random_state = 42, stratify=data['Star'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111518                                人物 出彩 萌物 … … 剧情 其实 无聊\n",
       "74565                                                    好看\n",
       "61763                      大话 之后 西游 这片 百分百 贺岁 档 凑个 热闹 不失 三星\n",
       "34109     土地公 可爱 适应 远 点 这种 人设 白龙 确实 从小到大 好看 白龙 特效 好看 没有 吹 神\n",
       "56680                                            数字   好看 一点\n",
       "Name: comment_processed, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "211157    画面 美 动物 萌 故事 精彩 喜欢 兔子 豹 豹 迪士尼 真的 太赞 zootopia 塑...\n",
       "132545    奇 怪 完 觉得 好看 笑点 不错 剧情 效果 诚意 中二 妖魔 地方 尴尬 本来   数字...\n",
       "13146                                           反正   数字   星\n",
       "73506                                                徐 克星 爷\n",
       "18189       有点 可爱 看不懂 梗 太 懂 梗 不萌 cp 剧情 弱 电影 一言不合 打架 一言不合 耍帅\n",
       "Name: comment_processed, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111518    1\n",
       "74565     1\n",
       "61763     1\n",
       "34109     1\n",
       "56680     1\n",
       "Name: Star, dtype: int32"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "211157    1\n",
       "132545    1\n",
       "13146     1\n",
       "73506     1\n",
       "18189     1\n",
       "Name: Star, dtype: int32"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111518                                人物 出彩 萌物 … … 剧情 其实 无聊\n",
       "74565                                                    好看\n",
       "61763                      大话 之后 西游 这片 百分百 贺岁 档 凑个 热闹 不失 三星\n",
       "34109     土地公 可爱 适应 远 点 这种 人设 白龙 确实 从小到大 好看 白龙 特效 好看 没有 吹 神\n",
       "56680                                            数字   好看 一点\n",
       "                                ...                        \n",
       "180862    以前 从来 不敢 丧尸 片 听说 釜山 行 好看 完 发现 丧尸 片 恐怖 反映 人性 结尾...\n",
       "153171                                 前   数字   分钟 尴尬 知道 干嘛\n",
       "104054        至少 流泪 片子 一个 渴望 安定 一个 渴望 自由 其实 七月 安生 一个 两面 离不开\n",
       "75596                                             一般般 没有 突破\n",
       "160596                                             其实 小说 不错\n",
       "Name: comment_processed, Length: 170004, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 把文本转换成向量的形式\n",
    "\n",
    "在这个部分我们会采用三种不同的方式:\n",
    "- 使用tf-idf向量\n",
    "- 使用word2vec\n",
    "- 使用bert向量\n",
    "\n",
    "转换成向量之后，我们接着做模型的训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 任务6：把文本转换成tf-idf向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(170004, 14587) (42502, 14587)\n"
     ]
    }
   ],
   "source": [
    "# TODO6: 把训练文本和测试文本转换成tf-idf向量。使用sklearn的feature_extraction.text.TfidfTransformer模块\n",
    "#    请留意fit_transform和transform之间的区别。 常见的错误是在训练集和测试集上都使用 fit_transform，需要避免！ \n",
    "#    另外，可以留意一下结果是否为稀疏矩阵\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "transformer = TfidfTransformer()\n",
    "\n",
    "x = vectorizer.fit_transform(comments_train)\n",
    "y = vectorizer.transform(comments_test)\n",
    "\n",
    "tfidf_train = transformer.fit_transform(x)\n",
    "tfidf_test = transformer.transform(y)\n",
    "print (tfidf_train.shape, tfidf_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 任务7：把文本转换成word2vec向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 由于训练出一个高效的word2vec词向量往往需要非常大的语料库与计算资源，所以我们通常不自己训练Wordvec词向量，而直接使用网上开源的已训练好的词向量。\n",
    "# data/sgns.zhihu.word是从https://github.com/Embedding/Chinese-Word-Vectors下载到的预训练好的中文词向量文件\n",
    "# 使用KeyedVectors.load_word2vec_format()函数加载预训练好的词向量文件\n",
    "model = KeyedVectors.load_word2vec_format('data/sgns.zhihu.word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.51068e-01,  2.57389e-01, -1.46752e-01, -4.45400e-03,\n",
       "       -1.04235e-01,  3.72475e-01, -4.29349e-01, -2.80470e-02,\n",
       "        1.56651e-01, -1.27600e-01, -1.68833e-01, -2.91350e-02,\n",
       "        4.57850e-02, -3.53735e-01,  1.61205e-01, -1.82645e-01,\n",
       "       -1.35340e-02, -2.42591e-01, -1.33356e-01, -1.31012e-01,\n",
       "       -9.29500e-02, -1.70479e-01, -2.54004e-01, -1.20530e-01,\n",
       "       -1.33690e-01,  7.84360e-02, -1.46603e-01, -2.77378e-01,\n",
       "       -1.36723e-01,  9.29070e-02, -4.00197e-01,  2.80726e-01,\n",
       "       -1.73282e-01,  8.56630e-02,  2.37251e-01,  6.24290e-02,\n",
       "       -1.57132e-01,  2.15685e-01,  9.54770e-02,  1.09896e-01,\n",
       "       -2.05394e-01, -3.37900e-03, -2.77480e-02,  8.16580e-02,\n",
       "        9.65290e-02,  1.23188e-01,  9.55090e-02, -2.31017e-01,\n",
       "       -8.59590e-02, -2.21634e-01, -1.37885e-01, -1.84790e-01,\n",
       "       -2.40127e-01, -2.79150e-01, -4.56200e-03,  1.04099e-01,\n",
       "        3.20523e-01, -6.77270e-02,  1.95719e-01,  4.06145e-01,\n",
       "       -2.98546e-01, -1.67750e-02,  2.74917e-01, -9.02350e-02,\n",
       "       -1.06762e-01, -2.47535e-01, -4.00415e-01,  2.06635e-01,\n",
       "        2.76320e-01, -3.13900e-03,  3.04576e-01,  1.17664e-01,\n",
       "       -2.17286e-01,  7.54650e-02, -1.44985e-01,  6.36960e-02,\n",
       "        1.58869e-01, -4.71568e-01, -1.08640e-01,  4.00144e-01,\n",
       "       -1.83435e-01,  1.88286e-01,  1.32482e-01, -8.50580e-02,\n",
       "       -8.65500e-03, -2.80691e-01, -1.10871e-01,  4.72890e-02,\n",
       "       -1.47635e-01, -5.17090e-02, -4.65100e-03, -1.73998e-01,\n",
       "       -6.15050e-02,  1.14153e-01,  7.09480e-02,  9.88670e-02,\n",
       "       -7.25230e-02,  4.64800e-02, -1.83534e-01, -1.97097e-01,\n",
       "       -7.94430e-02,  2.80280e-01, -2.44620e-01, -3.95528e-01,\n",
       "       -6.10930e-02, -2.53600e-01,  1.49320e-01,  2.82553e-01,\n",
       "        4.33800e-02,  3.50895e-01, -1.42657e-01, -9.72500e-03,\n",
       "       -1.38536e-01, -1.25489e-01, -1.06447e-01, -9.92880e-02,\n",
       "        4.94210e-02,  1.19487e-01, -6.15150e-02,  1.44710e-01,\n",
       "        1.85710e-01,  7.26870e-02,  1.90587e-01,  2.89779e-01,\n",
       "        2.03630e-01, -9.82690e-02,  1.36294e-01, -1.17514e-01,\n",
       "       -3.54500e-01,  3.30250e-02,  3.01922e-01, -6.46030e-02,\n",
       "       -2.21900e-03, -1.35516e-01,  1.81371e-01,  9.43760e-02,\n",
       "        2.73173e-01, -1.90694e-01,  1.20015e-01,  1.08732e-01,\n",
       "       -3.41390e-02,  1.17405e-01,  3.11844e-01, -8.31670e-02,\n",
       "        2.78229e-01,  3.37064e-01,  6.89230e-02,  2.01023e-01,\n",
       "        3.29060e-02, -4.36554e-01, -1.64540e-02,  2.31550e-02,\n",
       "       -1.96904e-01, -1.49370e-01,  7.83610e-02,  3.27980e-02,\n",
       "        2.42316e-01, -1.67102e-01,  2.93025e-01, -7.99780e-02,\n",
       "        5.57970e-02,  4.07600e-02, -1.87006e-01,  1.90802e-01,\n",
       "        1.10987e-01, -2.66690e-02, -1.09340e-01,  2.88753e-01,\n",
       "       -2.08372e-01,  6.85860e-02, -3.21254e-01,  6.55090e-02,\n",
       "       -2.84544e-01, -2.70365e-01,  2.22242e-01, -8.31220e-02,\n",
       "       -1.01721e-01,  3.11709e-01, -1.59856e-01,  3.19859e-01,\n",
       "        5.72180e-02,  3.15010e-01, -7.65140e-02,  3.07237e-01,\n",
       "        4.14023e-01,  9.61900e-02, -8.12400e-03,  3.59550e-01,\n",
       "       -1.05667e-01, -4.35740e-02,  1.97829e-01, -1.71804e-01,\n",
       "        1.21416e-01, -6.59890e-02,  3.14697e-01, -1.31049e-01,\n",
       "       -1.27306e-01, -4.13040e-02,  3.01799e-01, -2.47272e-01,\n",
       "        8.71550e-02, -4.88150e-01, -2.20991e-01,  4.65800e-02,\n",
       "       -1.34422e-01,  1.35731e-01, -1.72283e-01,  1.16328e-01,\n",
       "        2.88320e-02,  3.31440e-02,  9.48420e-02, -3.48560e-02,\n",
       "        7.54000e-02,  3.56407e-01, -2.56189e-01, -1.32000e-04,\n",
       "        1.05849e-01,  4.28803e-01,  2.86090e-02,  7.92700e-03,\n",
       "        3.58461e-01,  2.82804e-01, -5.88800e-02,  1.73850e-02,\n",
       "        9.28060e-02, -3.90392e-01,  1.89097e-01,  2.85916e-01,\n",
       "        1.51707e-01,  2.58823e-01,  1.63509e-01,  1.26390e-01,\n",
       "        1.95748e-01, -9.80750e-02,  9.12650e-02, -8.20320e-02,\n",
       "       -1.50282e-01,  1.10330e-01,  3.82834e-01, -1.21887e-01,\n",
       "       -1.31515e-01, -4.10777e-01,  2.19966e-01, -1.48785e-01,\n",
       "        1.02161e-01,  8.31420e-02,  2.08074e-01,  3.58526e-01,\n",
       "        1.41909e-01,  2.27764e-01,  4.61127e-01, -1.61267e-01,\n",
       "       -1.22107e-01,  1.02524e-01, -6.15770e-02,  2.10200e-02,\n",
       "        1.46990e-02, -2.23617e-01,  1.71110e-02,  1.20386e-01,\n",
       "       -5.65090e-02, -2.34566e-01,  4.34660e-02,  1.97851e-01,\n",
       "        2.37255e-01, -1.44901e-01,  4.41118e-01, -3.86210e-02,\n",
       "       -2.60820e-01,  4.17700e-02, -9.47700e-02,  3.21410e-02,\n",
       "       -1.86014e-01, -1.40884e-01,  2.02842e-01, -4.83673e-01,\n",
       "        2.19995e-01,  3.59395e-01, -1.84255e-01,  1.30998e-01,\n",
       "        1.10280e-01,  1.42483e-01, -2.01510e-01, -1.34156e-01,\n",
       "       -1.25440e-01, -9.89700e-02, -1.45869e-01, -2.23137e-01,\n",
       "        4.83180e-02,  2.55901e-01, -1.25977e-01, -1.36290e-01,\n",
       "       -3.33329e-01, -2.65370e-01, -1.48834e-01,  1.28487e-01,\n",
       "       -7.88080e-02,  1.35266e-01,  2.17841e-01,  6.60870e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#预训练词向量使用举例\n",
    "model['今天']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO7: 对于每个句子，生成句子的向量。具体的做法是：包含在句子中的所有单词的向量做平均。\n",
    "vocabulary = model.vocab\n",
    "\n",
    "# define the function to compute the mean vectors of sentences\n",
    "\n",
    "def get_average_word2vec(context, model, generate_missing=False, k=300):\n",
    "    if len(context)<1:\n",
    "        return np.zeros(k)\n",
    "    if generate_missing:\n",
    "        vectorized = [model[word] if word in model else np.random.rand(k) for word in context]\n",
    "    else:\n",
    "        vectorized = [model[word] if word in model else np.zeros(k) for word in context]\n",
    "    return np.mean(np.array(vectorized), axis=0)\n",
    "\n",
    "def get_word2vec_embeddings(model, tokens_comment, generate_missing=False):\n",
    "    embeddings = tokens_comment.apply(lambda x: get_average_word2vec(x, model, generate_missing=generate_missing))\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "word2vec_train = get_word2vec_embeddings(model, comments_train)\n",
    "word2vec_test = get_word2vec_embeddings(model, comments_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(170004, 300) (42502, 300)\n"
     ]
    }
   ],
   "source": [
    "print (word2vec_train.shape, word2vec_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 任务8：把文本转换成bert向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入gpu版本的bert embedding预训练的模型。\n",
    "# 若没有gpu，则ctx可使用其默认值cpu(0)。但使用cpu会使程序运行的时间变得非常慢\n",
    "# 若之前没有下载过bert embedding预训练的模型，执行此句时会花费一些时间来下载预训练的模型\n",
    "from bert_embedding import BertEmbedding\n",
    "import mxnet\n",
    "ctx = mxnet.cpu(0)\n",
    "embedding = BertEmbedding(ctx=ctx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function to compute the mean vectors of sentences\n",
    "\n",
    "def get_average_word2vec(context, model, k=768):\n",
    "    if len(context)<1:\n",
    "        return np.zeros(k)\n",
    "    empty = [' ']\n",
    "    vectorized = [model(word)[0][1][0] if word not in empty else np.zeros(k) for word in context ]\n",
    "    \n",
    "    return np.mean(np.array(vectorized), axis=0)\n",
    "\n",
    "def get_word2vec_embeddings(model, tokens_comment, generate_missing=False):\n",
    "    embeddings = tokens_comment.apply(lambda x: get_average_word2vec(x, model))\n",
    "    return np.vstack(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.97583673e-01, -2.87807945e-01, -2.32519265e-02,\n",
       "        -4.86667096e-01,  2.68476142e-01, -1.40509474e-01,\n",
       "         3.71786868e-01,  1.39615455e-01, -1.67486218e-01,\n",
       "        -3.39158001e-01, -4.53908113e-01, -1.93926794e-01,\n",
       "         1.54324765e-01, -1.83983590e-01, -2.20380197e-02,\n",
       "        -1.75222566e-01,  3.06705643e-01,  1.31488306e-01,\n",
       "         4.56508186e-01,  7.87235895e-01,  8.82213977e-02,\n",
       "         1.51522057e-01, -1.74331292e-01,  1.54278803e-02,\n",
       "         1.25916927e-01,  6.15206012e-01, -6.31442837e-02,\n",
       "        -5.71061437e-02, -2.59569860e-01,  2.53777579e-01,\n",
       "         3.73142107e-01, -1.36900248e-01, -1.27385543e-01,\n",
       "         2.91650135e-01, -2.30291171e-01, -3.05707358e-01,\n",
       "        -1.02974146e-02, -1.44511688e-01, -3.25359475e-01,\n",
       "         1.81648248e-01,  2.59573439e-01, -3.04074175e-01,\n",
       "         2.15310097e-01, -3.47099310e-01, -1.75572678e-01,\n",
       "        -5.65831511e-02,  2.56570424e-01,  1.17712395e-02,\n",
       "        -3.49093366e-01, -1.57048126e-02, -6.16551468e-01,\n",
       "        -4.03701165e-02,  1.66388587e-01,  1.66804325e-02,\n",
       "        -4.95159740e-01, -4.08252264e-02,  5.69175289e-01,\n",
       "         1.08295565e-01, -2.41923806e-01, -5.37119851e-01,\n",
       "         3.79687736e-02, -3.89432311e-01, -5.52364989e-04,\n",
       "        -7.25112515e-03,  7.11476815e-02,  2.88460002e-01,\n",
       "         5.79573334e-01,  3.67919790e-01, -6.30782999e-01,\n",
       "         4.97838058e-01, -1.43325697e-01,  1.75857466e-02,\n",
       "        -2.63153121e-01, -4.62732089e-02, -2.21698782e-02,\n",
       "         1.27078105e-01,  4.08506620e-03,  7.87525857e-02,\n",
       "        -3.55380705e-03, -2.93626330e-01,  1.37039731e-01,\n",
       "         1.05181440e-01, -1.28839878e-01,  1.94589003e-01,\n",
       "         8.19865329e-02, -6.13143454e-02,  8.94590559e-02,\n",
       "        -6.10519811e-02, -3.64881339e-01,  5.81420946e-01,\n",
       "        -1.48541705e-01, -2.31329845e-01, -1.86079103e-01,\n",
       "         6.71177172e-02,  1.23646859e-01, -3.23861296e-01,\n",
       "        -4.12114582e-01,  4.28283954e-01,  1.10999067e-01,\n",
       "        -6.93487287e-01, -4.24503164e-01, -6.72186476e-01,\n",
       "         2.23361118e-03,  3.88109444e-01, -4.58414551e-01,\n",
       "         2.44406065e-01, -3.42049249e-01, -2.96821622e-01,\n",
       "         8.11099165e-02,  1.04233690e-01,  7.13884553e-02,\n",
       "         2.14705240e-03,  2.76690750e-01, -2.03227966e-01,\n",
       "        -7.59300870e-02,  1.03001583e-01, -4.16301844e-01,\n",
       "        -3.30783683e-01, -1.38937385e-02,  3.93385332e-01,\n",
       "         1.83660234e-01, -4.16361869e-01, -6.28116997e-02,\n",
       "         6.55567739e-01,  2.38288879e-01,  8.00011671e-02,\n",
       "        -4.61801145e-01, -1.96268842e-02, -1.67782498e-01,\n",
       "         4.65871828e-02, -2.42297786e-01,  4.67332408e-01,\n",
       "         2.20408456e-01,  2.03566242e-01, -2.33697419e-01,\n",
       "        -2.33155964e-01, -1.04069206e-01, -6.79220424e-02,\n",
       "         1.51493130e-01, -6.88806467e-01,  3.49547768e-01,\n",
       "         6.51872677e-01,  7.92507939e-02, -6.63603725e-02,\n",
       "         2.45007780e-01, -3.96563740e-01, -4.90246997e-01,\n",
       "         4.89709730e-02,  6.19124347e-02, -8.79253100e-02,\n",
       "        -3.85560224e-01,  4.15955201e-01, -4.49395737e-02,\n",
       "        -7.20790782e-01, -4.87942560e-01, -3.94439201e-01,\n",
       "        -1.51870968e-01,  2.06935893e-01, -3.84288632e-01,\n",
       "         3.52897164e-01,  4.33618719e-01,  4.43131801e-01,\n",
       "        -1.08210284e-01,  5.31903533e-01,  3.35654727e-02,\n",
       "        -4.28215001e-02,  3.50868781e-02,  1.16163274e+00,\n",
       "        -8.15001325e-02, -3.74060373e-01, -5.25820130e-01,\n",
       "        -2.17970854e-01,  1.20960563e+00,  3.86408573e-01,\n",
       "        -8.09080025e-01,  9.77239495e-02,  3.67578518e-01,\n",
       "        -2.79049052e-01,  1.53630912e-01, -4.44164521e-02,\n",
       "        -2.66207645e-01,  2.85616560e-01, -3.00220884e-01,\n",
       "        -2.46792033e-01,  3.72047693e-01,  8.22699149e-02,\n",
       "         7.35270854e-01, -4.40783062e-01,  3.02160458e-01,\n",
       "         3.24082069e-01, -9.02888792e-01, -4.11555646e-01,\n",
       "        -3.40922092e-01, -1.16913572e-01, -5.43809035e-02,\n",
       "        -3.60448914e-01, -3.02508445e-01, -2.49911648e-01,\n",
       "         5.12582605e-01, -8.14009181e-02,  3.86675616e-01,\n",
       "        -1.42772606e-02,  4.21691139e-01,  7.79797156e-02,\n",
       "         5.13840040e-01,  2.48027698e-01,  3.45424448e-02,\n",
       "        -2.16032104e-01,  1.94164933e-01, -1.82231042e-01,\n",
       "        -8.35575526e-01,  6.95695706e-01,  5.11235979e-01,\n",
       "        -1.63388420e-02,  3.24027091e-01, -4.03081097e-01,\n",
       "         1.22827284e-01, -1.21562430e-01, -2.04998210e-01,\n",
       "         5.30219473e-01,  5.58361589e-01, -4.80951591e-01,\n",
       "        -8.48588404e-02,  2.59856932e-01, -7.30071815e-01,\n",
       "         2.84396662e-02,  2.77168465e-01, -6.72604768e-01,\n",
       "         8.85730643e-02, -1.36304277e-01, -1.61487635e-01,\n",
       "        -6.30713915e-01,  2.63911923e-01, -1.18701515e-01,\n",
       "        -5.02744670e-01,  4.37672976e-01,  1.19164967e-01,\n",
       "        -2.54852415e-01, -2.17734975e-02, -1.81573081e-01,\n",
       "        -1.92243909e-01,  1.42453537e-01,  6.27469405e-01,\n",
       "         3.62160898e-01, -2.85154259e-01,  3.52907850e-01,\n",
       "         5.93076438e-01, -3.05471917e-01,  5.59760746e-02,\n",
       "         2.44921616e-01, -2.30714395e-01, -5.29411948e-01,\n",
       "        -1.23080719e-01, -8.34796220e-02, -3.95603893e-01,\n",
       "        -3.02502012e-01, -3.77830691e-02, -1.97214040e-01,\n",
       "         2.04749063e-01,  4.29850436e-04,  2.37970470e-01,\n",
       "         8.37155449e-02,  1.22633738e+00, -3.07262189e-01,\n",
       "         8.03427921e-02,  1.78326473e-01,  4.46968746e-01,\n",
       "        -1.05217937e-01,  2.94102867e-02, -2.48663179e-01,\n",
       "        -2.37038136e-02, -8.30055764e-02,  2.17773861e-01,\n",
       "         6.19015481e-01, -2.25740693e-01, -4.45812430e-02,\n",
       "         2.65209262e-01,  1.75724889e-01, -9.51655836e-02,\n",
       "         1.23818855e-01,  3.09866303e-01,  1.70216238e-01,\n",
       "        -2.63702730e-01, -3.08602000e-01,  2.80953304e-01,\n",
       "        -1.37947371e-01, -4.46333580e-01, -7.01416190e-01,\n",
       "        -1.11213634e-03, -3.03891681e-01,  3.89046895e-03,\n",
       "         4.49261528e-01,  2.60283640e-01, -3.07858061e-01,\n",
       "         1.61821961e-01, -4.27094467e-02, -1.38109102e-01,\n",
       "         1.82377831e-01, -5.81672081e-02, -1.58785997e-02,\n",
       "        -2.15542003e-01, -6.62194984e-02, -4.03884904e-01,\n",
       "        -5.02312101e-01,  2.94383625e-01,  4.07585869e-01,\n",
       "        -1.50317312e-01,  2.78050255e-01, -2.00704864e+00,\n",
       "        -4.03847465e-01,  1.28251808e-01, -2.71144930e-01,\n",
       "         1.74784768e-01,  5.51373144e-02, -1.73333959e-01,\n",
       "         7.34722701e-02, -2.95676022e-01, -1.12764738e-01,\n",
       "         3.57525373e-01, -3.18405441e-01,  3.94007126e-01,\n",
       "         3.18907583e-01,  2.05030730e-01, -3.92706182e-01,\n",
       "         4.59331055e-01, -1.27487186e-01, -5.54542060e-01,\n",
       "         8.16935507e-01, -3.47599638e-01, -2.76741246e-01,\n",
       "         6.64832426e-01, -1.96712713e-01,  1.05238288e-01,\n",
       "         6.85200338e-01,  2.92072400e-02,  4.36386904e-01,\n",
       "        -7.19187056e-01, -3.18443919e-01, -3.10686950e-01,\n",
       "        -4.91647339e-01,  4.09453050e-01,  1.72892982e-01,\n",
       "         1.51286632e-01,  5.11371322e-01,  4.22843969e-01,\n",
       "        -5.84872570e-01,  1.17966870e+00, -7.78292094e-02,\n",
       "         4.04520673e-01, -3.85689054e-01,  2.42663283e-01,\n",
       "         3.25333825e-01,  2.28054332e-01, -2.55650820e-01,\n",
       "         3.69673991e-01,  5.68938394e-02,  1.41470516e-01,\n",
       "         3.48936815e-02,  2.27894425e-01, -2.51356334e-01,\n",
       "        -3.65241183e-01, -1.38609087e-01, -4.98977301e-02,\n",
       "         1.57826761e-01,  1.85217445e-02,  1.36985402e-01,\n",
       "        -8.57692248e-02, -2.25386075e-01,  3.83321097e-01,\n",
       "         3.05011813e-02, -4.96476639e-01,  6.58478114e-01,\n",
       "        -9.38855184e-01,  3.04825630e-02, -5.60178815e-01,\n",
       "        -4.05828928e-01, -1.91408817e-01,  2.01913180e-01,\n",
       "        -2.37625515e-01, -1.39254157e-01,  1.93328333e-01,\n",
       "        -5.77267801e-01, -3.87721603e-01,  2.53867820e-01,\n",
       "        -2.71576124e-01, -4.97666448e-01,  1.54923414e-01,\n",
       "        -8.85200834e-01, -1.14865832e-01, -3.50319657e-02,\n",
       "         2.56857945e-01,  3.64845376e-02,  1.37310885e-01,\n",
       "        -2.73847690e-02,  7.20882789e-01, -4.30134321e-01,\n",
       "        -1.33161379e-01, -1.72932074e-02,  7.49449471e-02,\n",
       "         4.48422513e-01,  4.48285818e-01,  2.76527589e-01,\n",
       "         5.04962268e-01,  6.51083972e-02,  4.31348051e-01,\n",
       "        -3.85284862e-01,  2.11062445e-01,  2.62960050e-02,\n",
       "        -3.72788297e-01, -2.10668090e-01, -1.28557909e-01,\n",
       "         1.74733596e-01, -9.18783991e-03, -4.36338666e-02,\n",
       "        -1.01906515e+00,  8.68657510e-02, -3.24769282e-02,\n",
       "        -1.63957451e-01,  3.05419739e-01, -2.60430494e-02,\n",
       "         1.96681099e-02,  3.13771751e-01, -5.00968436e-01,\n",
       "        -1.62892683e-01, -1.36796132e-01, -2.04278596e-01,\n",
       "        -7.64688664e-02,  8.62776084e-02, -1.14741787e-01,\n",
       "         5.00401505e-01, -5.87339497e-01, -2.35257260e-01,\n",
       "        -5.35056576e-02,  2.57919152e-01, -9.79168373e-02,\n",
       "        -3.26747741e-01, -6.26974419e-01,  3.35474838e-01,\n",
       "         1.61425560e-01, -2.62357033e-01,  1.38501742e-01,\n",
       "         9.80192561e-03, -3.88861752e-01,  3.90588334e-02,\n",
       "        -1.71524510e-01,  1.74173236e-01, -2.85293872e-01,\n",
       "         6.17029469e-01,  5.84128758e-01,  2.21314495e-02,\n",
       "        -5.99327479e-01,  9.84294297e-02,  3.37940464e-01,\n",
       "         3.47841106e-01, -2.55839369e-01,  5.67262283e-02,\n",
       "         6.13107948e-02,  6.69688627e-01,  3.35927028e-01,\n",
       "        -7.97486622e-01,  4.40928063e-01,  1.50416316e-01,\n",
       "        -1.70359281e-01, -4.75728902e-01,  5.97568148e-02,\n",
       "        -4.13298730e-01,  2.44346384e-01, -2.12621894e-01,\n",
       "         2.62155226e-01, -5.28639637e-02,  3.93285150e-01,\n",
       "         4.21314857e-01,  4.77870167e-01,  6.92952807e-01,\n",
       "         9.70858030e-03,  2.05259568e-01, -3.49538750e-01,\n",
       "         6.38959566e-02,  3.94845030e-01, -4.49964746e-01,\n",
       "         1.89024279e-01,  2.48435518e-01, -3.06927805e-01,\n",
       "        -2.25206480e-01, -5.91221953e-02, -5.63951006e-01,\n",
       "        -1.95631928e-01, -8.99968503e-03,  3.79276864e-01,\n",
       "         3.75848344e-02, -1.12294027e-02, -1.39725892e-01,\n",
       "         4.51366128e-01, -4.88143379e-02, -6.82127239e-01,\n",
       "         1.32459790e-02,  4.79457919e-01,  4.33428091e-01,\n",
       "         3.20194935e-01, -5.88190345e-01,  5.22834102e-02,\n",
       "        -3.59022036e-01, -6.92833195e-02, -3.95855679e-01,\n",
       "         2.36157995e-01,  2.95031717e-01,  6.20029021e-02,\n",
       "         6.20841317e-01,  1.78401934e-01, -4.98807227e-01,\n",
       "         9.67657574e-02, -8.46380894e-03, -4.89899180e-03,\n",
       "         5.84725771e-02,  1.32824458e-01,  4.07782917e-01,\n",
       "        -2.72381868e-01, -2.82456378e-01,  5.25854336e-01,\n",
       "         5.37257917e-03, -4.43075041e-01, -9.39912499e-02,\n",
       "        -5.02060178e-01,  6.36844899e-01, -3.03006082e-02,\n",
       "         9.55036959e-02,  1.37381827e-01, -5.75656634e-01,\n",
       "        -4.02588662e-01, -5.00137163e-01,  1.97449735e-01,\n",
       "        -3.19960764e-02,  9.22546615e-02, -1.02619215e-01,\n",
       "        -4.77372513e-01, -5.59416867e-01,  2.17699411e-01,\n",
       "        -4.09269041e-01,  4.93394187e-03,  2.57574267e-01,\n",
       "        -9.91999378e-02, -1.02013432e-01,  2.86417881e-01,\n",
       "        -1.18968722e-01, -6.55593165e-01, -1.96741805e-01,\n",
       "        -4.23632503e-01, -1.16038189e-01,  1.97708446e-02,\n",
       "        -8.20661162e-01, -5.69777152e-01,  7.44301808e-02,\n",
       "        -3.05797305e-01,  8.86842951e-02,  4.89959868e-02,\n",
       "        -1.18599619e-01,  9.14745394e-01,  9.60562842e-01,\n",
       "        -4.08874229e-01,  4.92681435e-01,  6.35613296e-02,\n",
       "        -1.05755860e-01,  5.66622551e-03, -7.27830316e-01,\n",
       "        -1.85776467e-01,  4.75081066e-02,  1.88810337e-01,\n",
       "         2.23764779e-01, -1.13316426e-01,  6.32123908e-01,\n",
       "         7.34893860e-02, -6.71954738e-02, -2.78018622e-01,\n",
       "         1.96176132e-01, -2.63995712e-01,  3.65661546e-01,\n",
       "         5.04210144e-01,  2.24022856e-01, -6.78668880e-01,\n",
       "        -2.67001896e-02,  1.57286010e-01,  2.15360129e-01,\n",
       "        -2.06269711e-01, -1.16390304e-01,  1.04643371e-01,\n",
       "        -5.89102837e-01, -6.26522312e-02,  5.92485246e-02,\n",
       "         1.82798951e-01,  9.64358510e-01,  2.18951259e-01,\n",
       "         1.86331392e-01,  8.13125781e-01, -4.66528835e-01,\n",
       "        -4.47785835e-01, -1.49925650e-01, -2.10628852e-01,\n",
       "        -6.66480223e-01, -6.04397736e-02,  1.74249865e-01,\n",
       "         2.07936175e-03,  2.98904023e-01,  1.94026170e-01,\n",
       "         4.69104156e-01, -5.35207866e-01,  2.06629473e-01,\n",
       "        -8.63366851e-01, -6.41010626e-02, -8.23581605e-02,\n",
       "        -1.36954259e-01, -4.63268563e-02,  4.79005770e-02,\n",
       "        -3.60261450e-01,  5.63384995e-01,  2.17786824e-01,\n",
       "        -3.39277328e-01,  1.24528374e-01,  5.95302439e-02,\n",
       "        -6.34950497e-01, -1.40052988e-01, -8.57032068e-02,\n",
       "         7.19289008e-01,  1.96243923e-02, -2.53623760e-01,\n",
       "        -3.96457538e-02, -7.88450050e-01, -1.29975448e-01,\n",
       "         4.22143464e-01, -1.05351939e-01,  4.31983179e-02,\n",
       "         5.57934689e-01,  5.23574425e-01,  6.05152975e-02,\n",
       "        -1.76871009e-01, -3.29905267e-01,  2.01730925e-01,\n",
       "         2.29581261e-01,  8.64760753e-01,  1.61916854e-01,\n",
       "        -1.67724829e-01,  2.36558325e-01,  2.86008004e-01,\n",
       "         6.65527355e-02, -4.81344725e-01,  3.94885527e-01,\n",
       "         6.98981557e-02, -3.69837010e-01, -4.07889553e-01,\n",
       "        -2.36088571e-01,  1.51838751e-01,  4.76791662e-01,\n",
       "        -2.79779588e-01, -1.24932103e-01, -6.79414195e-02,\n",
       "         2.98979252e-01, -4.66162371e-01,  1.37799772e-01,\n",
       "         6.97110687e-01,  5.47951690e-02, -1.39078842e-01,\n",
       "        -6.90465866e-01,  1.92211387e-01, -5.41459239e-02,\n",
       "         1.55979496e-01,  3.64699762e-02,  1.81213712e-01,\n",
       "         3.86004733e-03,  3.06768005e-01,  1.29649364e-01,\n",
       "         5.24290226e-01,  3.72866070e-01, -1.62659527e-01,\n",
       "        -1.21363234e-01, -2.69208459e-02, -1.52041225e-01,\n",
       "        -4.20183727e-01, -1.36243355e-01, -2.63877125e-01,\n",
       "        -6.51769761e-02,  4.35603279e-01,  3.09397204e-01,\n",
       "         3.13128186e-01,  1.56408747e-02,  1.12615578e-01,\n",
       "        -2.44424860e-01,  4.22691329e-01, -7.88444084e-01,\n",
       "        -3.69472336e-01, -4.49237454e-04,  9.08955027e-02,\n",
       "        -9.86108356e-02, -3.63242227e-01,  7.41473785e-01,\n",
       "        -5.94994118e-01,  6.67891894e-03,  1.54109981e-02,\n",
       "         2.15014188e-01, -4.83278379e-02,  1.02304414e-01,\n",
       "        -3.96280198e-02, -2.21493953e-01, -2.04033402e-01,\n",
       "         4.72847486e-01, -2.22593463e-01, -2.78261252e-01,\n",
       "        -8.73157763e-02,  6.34092815e-01,  3.23090600e-01,\n",
       "         6.43241826e-01, -3.46511377e-01,  1.35108007e-01,\n",
       "         1.34759416e-02, -5.20855841e-01, -2.73570193e-02,\n",
       "         2.29400688e-01,  3.36350090e-01, -4.32498287e-01,\n",
       "        -6.85259419e-01,  7.19545089e-01,  4.59247774e-01,\n",
       "        -2.57788093e-01,  6.15235123e-01, -3.71046436e-02,\n",
       "        -4.26686987e-01,  1.67690860e-01, -4.58832928e-01,\n",
       "        -1.42985701e-02, -6.74139774e-02, -4.00301579e-02,\n",
       "        -2.73199481e-01, -8.27166707e-02,  1.62937714e-01,\n",
       "        -1.20462017e-01, -3.22031294e-01, -5.43197567e-01,\n",
       "         1.99121959e-01, -1.93673002e-01,  5.15078440e-01,\n",
       "        -3.25215992e-01,  4.55402425e-01,  6.02649282e-02,\n",
       "         1.95419477e-01, -1.45931423e-01, -4.59676826e-01,\n",
       "         8.45484323e-02, -3.81053566e-01, -3.36163930e-02,\n",
       "        -1.92496919e-01,  2.59834653e-01,  5.35814630e-01,\n",
       "        -2.66553106e-01, -5.47145162e-01,  3.47747378e-01,\n",
       "        -3.87115661e-02, -6.04934150e-01, -1.19008379e-01,\n",
       "        -8.79321411e-02, -2.94045759e-01,  5.23315747e-01,\n",
       "        -1.68410961e-01,  3.02238211e-01, -6.24420817e-02,\n",
       "        -2.31644039e-01, -2.43306883e-01,  2.96654921e-01,\n",
       "         1.16301348e-01,  2.45740957e-01, -1.82310716e-01]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#我的码单跑一行是可以的，但是拿CPU跑太慢了，实在跑不出来\n",
    "\n",
    "first_row_test=get_word2vec_embeddings(embedding, comments_test[0:1])\n",
    "first_row_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO8: 跟word2vec一样，计算出训练文本和测试文本的向量，仍然采用单词向量的平均。\n",
    "\n",
    "#bert_train=get_word2vec_embeddings(embedding, comments_train)\n",
    "#bert_test=get_word2vec_embeddings(embedding, comments_test)\n",
    "#print (bert_train.shape, bert_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(170004, 14587) (42502, 14587)\n",
      "(170004, 300) (42502, 300)\n"
     ]
    }
   ],
   "source": [
    "print (tfidf_train.shape, tfidf_test.shape)\n",
    "print (word2vec_train.shape, word2vec_test.shape)\n",
    "#print (bert_train.shape, bert_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 训练模型以及评估\n",
    "对如上三种不同的向量表示法，分别训练逻辑回归模型，需要做：\n",
    "- 搭建模型\n",
    "- 训练模型（并做交叉验证）\n",
    "- 输出最好的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入逻辑回归的包\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 任务9：使用tf-idf，并结合逻辑回归训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuned hpyerparameters :(best parameters)  {'C': 0.01}\n",
      "accuracy : 0.829580476234962\n"
     ]
    }
   ],
   "source": [
    "# TODO9: 使用tf-idf + 逻辑回归训练模型，需要用gridsearchCV做交叉验证，并选择最好的超参数\n",
    "\n",
    "param_grid = {'C': np.arange(0.01, 0.1, 1)}\n",
    "fit_model = LogisticRegression()\n",
    "logreg = GridSearchCV(fit_model, param_grid, cv=10)\n",
    "logreg.fit(tfidf_train, y_train)\n",
    "print(\"tuned hpyerparameters :(best parameters) \", logreg.best_params_)\n",
    "print(\"accuracy :\", logreg.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF LR test accuracy 0.8305726789327561\n",
      "TF-IDF LR test F1_score 0.4748055092330618\n"
     ]
    }
   ],
   "source": [
    "tfidflog = LogisticRegression(C = 0.01)\n",
    "tfidflog.fit(tfidf_train, y_train)\n",
    "tf_idf_y_pred = tfidflog.predict(tfidf_test)\n",
    "\n",
    "print('TF-IDF LR test accuracy %s' % metrics.accuracy_score(y_test, tf_idf_y_pred))\n",
    "#逻辑回归模型在测试集上的F1_Score\n",
    "print('TF-IDF LR test F1_score %s' % metrics.f1_score(y_test, tf_idf_y_pred,average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 任务10：使用word2vec，并结合逻辑回归训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuned hpyerparameters :(best parameters)  {'C': 0.01}\n",
      "accuracy : 0.8326392319482936\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'C': np.arange(0.01, 0.1,1)}\n",
    "fit_model = LogisticRegression()\n",
    "word2vec_log = GridSearchCV(fit_model, param_grid, cv=10)\n",
    "word2vec_log.fit(word2vec_train, y_train)\n",
    "\n",
    "print(\"tuned hpyerparameters :(best parameters) \", word2vec_log.best_params_)\n",
    "print(\"accuracy :\", word2vec_log.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2vec LR test accuracy 0.8334196037833513\n",
      "Word2vec LR test F1_score 0.5104312765711536\n"
     ]
    }
   ],
   "source": [
    "# TODO10: 使用word2vec + 逻辑回归训练模型，需要用gridsearchCV做交叉验证，并选择最好的超参数\n",
    "word2vec_logreg = LogisticRegression(C = 0.01)\n",
    "word2vec_logreg.fit(word2vec_train, y_train)\n",
    "word2vec_y_pred = word2vec_logreg.predict(word2vec_test)\n",
    "\n",
    "\n",
    "print('Word2vec LR test accuracy %s' % metrics.accuracy_score(y_test, word2vec_y_pred))\n",
    "#逻辑回归模型在测试集上的F1_Score\n",
    "print('Word2vec LR test F1_score %s' % metrics.f1_score(y_test, word2vec_y_pred,average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 任务11：使用bert，并结合逻辑回归训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO11: 使用bert + 逻辑回归训练模型，需要用gridsearchCV做交叉验证，并选择最好的超参数\n",
    "#param_grid = {'C': np.arange(0.01, 0.1,1)}\n",
    "#fit_model = LogisticRegression()\n",
    "#bert_log = GridSearchCV(fit_model, param_grid, cv=10)\n",
    "#bert_log.fit(word2vec_train, y_train)\n",
    "\n",
    "\n",
    "#print('Bert LR test accuracy %s' % metrics.accuracy_score(y_test, bert_y_pred))\n",
    "#逻辑回归模型在测试集上的F1_Score\n",
    "#print('Bert LR test F1_score %s' % metrics.f1_score(y_test, bert_y_pred,average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 任务12：对于以上结果请做一下简单的总结，按照1，2，3，4提取几个关键点，包括：\n",
    "- 结果说明什么问题？\n",
    "- 接下来如何提高？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\n",
    "2.\n",
    "3.\n",
    "4.\n",
    "5.\n",
    "6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预处理后的tf-idf和bert生成向量模型的准确率都是在83%左右，但macroF1-score只在50%左右。有可能是样本分布不均导致模型F1较低。\n",
    "#另外，有可能相同的词汇多种语义会对y产生不同的影响。\n",
    "#除此之外，我们也可以应用Navie Bayes或者LDA等其他模型来预测评论等级来提高模型。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
